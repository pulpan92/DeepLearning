{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[본 내용은 밑바닥부터 시작하는 딥러닝을 바탕으로 작성하였습니다.]\n",
    "# 신경망 학습\n",
    "\n",
    "- - -\n",
    "\n",
    "## 데이터 학습\n",
    "\n",
    "신경망의 특징은 사람이 아닌 *데이터*를 중심으로 학습할 수 있다는 점이다. 예를 들어 이전 MNIST 데이터의 image를 보고 사람은 바로 이 숫자가 어떤 것인지 알 수 있지만 컴퓨터가 알아볼 수 있게 적용시킬 수 있는 알고리즘을 생각해내는 것은 어려운 일이다.\n",
    "\n",
    "따라서 알고리즘을 처음부터 설계하는 방식보다는 데이터를 *학습*하여 알고리즘을 변형시켜나가는 것이 더 효과적이다. 따라서 이미지 데이터에서 **특징**을 추출하여 그 패턴을 기계학습 기술로 학습할 수 있다. 보통 이미지를 처리할때는 벡터로 변환한 후 변환된 벡터 데이터를 가지고 supervised learning을 사용한다. (SVM, KNN)\n",
    "\n",
    "신경망과의 차이점은 사람이 생각한 특징을 먼저 vector로 뽑아내야 하는데 여기서 사람의 판단이 개입된다는 것이다. 하지만 신경망을 구성한다면 해당 특징을 추출해내고 분류해내는 것 모두 기계가 담당할 것이다.\n",
    "\n",
    "\n",
    "### 데이터\n",
    "\n",
    "기계학습 문제는 training data와 test data로 나눠 학습과 실험을 수행하는 것이 일반적이다. 먼저 train data를 사용하여 최적의 매개변수를 찾고 이후 test data를 통해 이 매개변수의 정확성을 측정하는 방식이다. \n",
    "\n",
    "하지만 train data에 지나치게 최적화된 상태를 *오버 피팅*이라고 하며 학습의 횟수를 잘 정해 이러한 상태를 피해야 한다.\n",
    "\n",
    "- - -\n",
    "\n",
    "## Loss function\n",
    "\n",
    "신경망에서 사용하는 Loss function은 최적의 Weight 값을 찾아내기 위해서 사용하는 지표이다.\n",
    "\n",
    "### 평균 제곱 오차\n",
    "\n",
    "\n",
    "![Alt text](4_neuralnet_pulpan92_image/image1.png)\n",
    "\n",
    "가장 많이 쓰이는 Loss function으로 수식은 위와 같이 나타난다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] pred right:  0.09750000000000003\n",
      "[] pred_fail:  0.6475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss_square(pred, ans):\n",
    "    return 0.5 * np.sum((pred - ans)**2)\n",
    "\n",
    "pred_right = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "pred_fail = np.array([0.1, 0.05, 0.05, 0.0, 0.6, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "\n",
    "ans = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "print('[] pred right: ', loss_square(pred_right, ans))\n",
    "print('[] pred_fail: ', loss_square(pred_fail, ans))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function을 구현한 것이다. pred_? 는 신경망에서 나온 각 정답 확률이고 ans는 index 2가 정답이라는 것을 나타낸다 pred_right은 해당 index에 확률 0.6을 부여하여 정답이 맞게 예측을 했지만 pred_fail은 다른 index의 확률이 더 높다고 판단하였다.\n",
    "\n",
    "각각의 cost 값을 보게되면 pred_fail의 경우는 cost 값이 크게 나온다. 따라서 이러한 식으로 예측값과 실제 정답의 오차를 구할 수 있다.\n",
    "\n",
    "\n",
    "### 교차 엔트로피 오차\n",
    "\n",
    "\n",
    "![Alt text](4_neuralnet_pulpan92_image/image2.png)\n",
    "\n",
    "평균 제곱 오차와 같이 교차 엔트로피 오차도 많이 쓰인다. 식은 위와 같다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.1, 10, 0.00001)\n",
    "elog = np.log(x)\n",
    "melog = -np.log(x)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "l1 = fig.add_subplot(1, 2, 1)\n",
    "l2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "l1.set_xlim(0, 1)\n",
    "l2.set_xlim(0, 1)\n",
    "\n",
    "l1.plot(x, elog)\n",
    "l2.plot(x, melog)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연 로그의 그래프는 위와 같이 나온다. 위의 식에서 yk는 신경망의 출력, tk는 정답레이블이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] pred_right:  0.5108239571007129\n",
      "[] pred_fail:  13.815510557964274\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(y, t):\n",
    "    d = 1e-6\n",
    "    return -np.sum(t * np.log(y + d))\n",
    "\n",
    "ans = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "pred_right = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "pred_fail = np.array([0.1, 0.05, 0.0, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.6])\n",
    "\n",
    "print('[] pred_right: ', cross_entropy(pred_right, ans))\n",
    "print('[] pred_fail: ', cross_entropy(pred_fail, ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아까와 같은 input으로 실험을 했을 때 위와 같은 cost 값이 나오게 된다. 손실함수란 결국 내가 예측한 값과 실제 데이터의 정답 사이의 차이를 이끌어내는 함수를 말한다. 그리고 이것을 가장 최소화 시키는 것이 신경망의 **정확도**를 높이는 방법이다.\n",
    "\n",
    "그리고 이러한 과정을 위해서 cost function의 값을 최대한 작게하는 가장 알맞은 weight값을 찾으려 미분값을 단서로 해당 매개변수 값을 갱신하는 과정을 반복하게 된다. 이러한 cost 값을 미분하게 되면 그 값은 매개변수의 값의 변화량을 나타내기때문에 음수면 그 매개변수를 양의 방향으로 변화시켜 cost 값을 줄일 수 있다. \n",
    "\n",
    "따라서 결과적으로 미분 값이 0이면 가중치의 매개변수의 변화량이 더이상 없다는 뜻이기 때문에 갱신은 멈추게 된다. \n",
    "\n",
    "그리고 cost function을 매개변수의 최적화의 지표로 삼는 이유는 정확도를 지표로 삼았을 시 미분값이 매개변수의 미세한 변화에 따라 연속적으로 움직이지 않기 때문이다.\n",
    "\n",
    "\n",
    "- - - \n",
    "\n",
    "## 수치 미분\n",
    "\n",
    "미분 값은 어떠한 수식에서부터 그 순간의 변화량을 구한 값이다. \n",
    "\n",
    "![Alt text](4_neuralnet_pulpan92_image/image3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def func(x):\n",
    "    return 0.01 * (x ** 2) + (0.1 * x)\n",
    "\n",
    "x = np.arange(0, 10, 0.01)\n",
    "y = func(x)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid()\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 그래프로 나타낸다면 위와 같이 나타난다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] x == 5 diff:  0.1999999999990898\n"
     ]
    }
   ],
   "source": [
    "def diff(x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    return (func(x + h) - func(x - h)) / (2 * h)\n",
    "\n",
    "\n",
    "print('[] x == 5 diff: ', diff(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차분 방법을 이용하여 해당 x값의 기울기를 구할 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 편미분\n",
    "\n",
    "\n",
    "![Alt text](4_neuralnet_pulpan92_image/image4.png)\n",
    "\n",
    "앞의 단순한 미분과 달리 미분되는 변수가 2개라는 점이 다르다.\n",
    "\n",
    "\n",
    "- - -\n",
    "\n",
    "\n",
    "## 기울기\n",
    "\n",
    "만약 편미분을 동시에 수행한다면 각자의 변피분을 벡터로 정의한 것을 **기울기** 라고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def func(x):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "def gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(x.size):\n",
    "        tmp_val = x[i]\n",
    "        \n",
    "        x[i] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[i] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[i] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[i] = tmp_val\n",
    "        \n",
    "    return grad\n",
    "\n",
    "x = np.array([1.0, 1.0])\n",
    "print(gradient(func, x))\n",
    "\n",
    "x = np.array([2.0, 2.0])\n",
    "print(gradient(func, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 code로 각 점의 편미분 기울기를 구할 수 있다. \n",
    "\n",
    "![Alt text](4_neuralnet_pulpan92_image/image5.png)\n",
    "\n",
    "편미분의 기울기 그래프는 위와 같이 나타나는데 사실 기울기라는 것은 가장 낮아지는 방향으로 기울어진다. 더 정확한 표현으로는 **기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향** 을 뜻한다.\n",
    " \n",
    "### 경사법\n",
    "\n",
    "신경망 역시 최적의 weight 값을 학습 시에 찾아야 한다. 하지만 매개변수의 공간이 넓기 때문에 cost function이 최솟값이 되는 매개변수 값이 무엇인지를 잘 모른다. 그리고 실제 복잡한 함수에서는 기울기 값이 가리키는 방향에 최솟값이 없는 경우가 많다.\n",
    "\n",
    "이럴 때 **경사법** 을 사용하게 되는데 이 방식은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동하다가 그 부분에서 기울어진 방향으로 일정거리를 이동 이 과정을 반복하는 방식이 **경사법**이다.\n",
    "\n",
    "![Alt text](4_neuralnet_pulpan92_image/image6.png)\n",
    "\n",
    "에타 기호는 갱신하는 양을 나타내며 **학습률**이라고도 한다. 따라서 매개변수 값을 얼마나 갱신하느냐를 정하게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.68296736e-09 1.68296736e-09]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def func(x):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "def gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(x.size):\n",
    "        tmp_val = x[i]\n",
    "        \n",
    "        x[i] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[i] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[i] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[i] = tmp_val\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def gradient_descent(f, init_x, R = 0.01, step = 20):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(0, step):\n",
    "        grad = gradient(f, x)\n",
    "        x -= R * grad\n",
    "     \n",
    "    return x\n",
    "\n",
    "x = np.array([1.0, 1.0])\n",
    "print(gradient_descent(func, x, 0.01, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사 하강법을 이용한 최적화된 Weight값을 구하는 code이다. \n",
    "\n",
    "### 신경망에서의 기울기\n",
    "\n",
    "경사 하강법을 신경망 학습에다 적용시켜 최적의 weight 값을 구해야 한다. \n",
    "\n",
    "![Alt text](4_neuralnet_pulpan92_image/image7.png)\n",
    "\n",
    "(2, 3) 형태의 가중치(W)가 존재할 때 손실함수는 L, 그리고 경사는 위와 같이 나타난다. (1, 1)에 있는 정보는 w11의 변화량에 따라 L의 변화량이 얼마나 되는지를 나타낸다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] predict:  [ 0.94411311 -0.29049154 -0.38802325]\n",
      "[] Loss:  1.773522958990222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def func(x):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def cross_err(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def gradient(f, x):\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "    h = 1e-4\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        t = x[idx]\n",
    "        \n",
    "        x[idx] = t + h\n",
    "        fxh1 = f(x)\n",
    "        print('[] fxh1: ', fxh1)\n",
    "        \n",
    "        x[idx] = t - h\n",
    "        fxh2 = f(x)\n",
    "        print('[] fxh2: ', fxh2)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = t\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "class simplenet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "#         print(self.W)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_err(y, t)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "net = simplenet()\n",
    "#print(net.W.shape)\n",
    "\n",
    "x = np.array([0.6, 0.1])\n",
    "t = np.array([0, 0, 1])\n",
    "print('[] predict: ', net.predict(x))\n",
    "print('[] Loss: ', net.loss(x, t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 특정 답에 대한 X의 오류를 구할 수 있다. \n",
    "\n",
    "\n",
    "\n",
    "- - -\n",
    "\n",
    "## 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Net:\n",
    "    def __init__(self, input_size, hidden_size, output_size, R = 0.01):\n",
    "        self.layer = {} \n",
    "        self.layer['W1'] = R * np.random.randn(input_size, hidden_size)\n",
    "        self.layer['b1'] = np.zeros(hidden_size)\n",
    "        self.layer['W2'] = R * np.random.randn(hidden_size, output_size)\n",
    "        self.layer['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def _numerical_gradient_no_batch(f, x):\n",
    "        h = 1e-4  # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "    \n",
    "        for idx in range(x.size):\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x)\n",
    "        \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "            x[idx] = tmp_val\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "    def numerical_gradient(f, X):\n",
    "        if X.ndim == 1:\n",
    "            return _numerical_gradient_no_batch(f, X)\n",
    "        else:\n",
    "            grad = np.zeros_like(X)\n",
    "            \n",
    "            for idx, x in enumerate(X):\n",
    "                grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "            \n",
    "            return grad\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            max = np.max(x, 0)\n",
    "            \n",
    "            # nomalization\n",
    "            x = x - max\n",
    "            res = np.exp(x) / np.sum(np.exp(x), 0)\n",
    "            print('[] ndim 2: ', res)\n",
    "            return res.T\n",
    "        \n",
    "        x = x - np.max(x)\n",
    "        res = np.exp(x) / np.sum(np.exp(x))\n",
    "        print('[] ndim 1: ', res)\n",
    "        \n",
    "        return res\n",
    "            \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.layer['W1'], self.layer['W2']\n",
    "        b1, b2 = self.layer['b1'], self.layer['b2']\n",
    "        \n",
    "        L1 = np.dot(x, W1) + b1\n",
    "        Z1 = self.sigmoid(L1)\n",
    "        \n",
    "        L2 = np.dot(Z1, W2) + b2\n",
    "        Z2 = self.sigmoid(L2)\n",
    "        \n",
    "        self.res = Z2\n",
    "        self.res = self.softmax(self.res)\n",
    "        \n",
    "        return self.res\n",
    "    \n",
    "    def cross_entropy_error(y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        \n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "             \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, pred, t):\n",
    "        y_argmax = np.argmax(pred, 1)\n",
    "        t_argmax = np.argmax(t, 1)\n",
    "        \n",
    "        print('[] y_argmax: ', y_argmax)\n",
    "        print('[] t_argmax: ', t_argmax)\n",
    "        \n",
    "        accu = np.sum(y_argmax == t_argmax) / float(x.shape[0])\n",
    "        return accu\n",
    "    \n",
    "    def numer_gradient(self, x, t):\n",
    "        cost = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numer_gradient(cost, self.params['W1'])\n",
    "        grads['b1'] = numer_gradient(cost, self.params['b1'])\n",
    "        grads['W2'] = numer_gradient(cost, self.params['W2'])\n",
    "        grads['b2'] = numer_gradient(cost, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    \n",
    "\n",
    "def main():\n",
    "    NN = Net(784, 100, 10)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서는 학습을 진행하기 위한 필요한 메소드들을 정리해놓은 class이다. 학습이 진행될수록 grads에는 기울기의 정보가, params 변수에는 weight 값이 저장되게 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numerical_gradient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-dd28aced08c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mt_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-dd28aced08c9>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'numerical_gradient' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.chdir('D:\\Machine Learning\\deeplearning\\example')\n",
    "from dataset.mnist import *\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "\n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    \n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "train_loss_list = []\n",
    "\n",
    "it = 10\n",
    "train_size = x_train.shape[0]\n",
    "total_batch = 100\n",
    "R = 0.1\n",
    "\n",
    "net = TwoLayerNet(784, 50, 10)\n",
    "\n",
    "for i in range(0, it):\n",
    "    # choice random image index\n",
    "    batch = np.random.choice(train_size, total_batch)\n",
    "    x_batch = x_train[batch]\n",
    "    t_batch = t_train[batch]\n",
    "    \n",
    "    grad = net.numerical_gradient(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "machine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
