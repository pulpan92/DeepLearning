{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활성화 함수\n",
    "\n",
    "\n",
    "### sigmoid 함수\n",
    "\n",
    "\n",
    "![Alt text](3_function_pulpan92_image/image1.png)\n",
    "\n",
    "sigmoid 함수는 어떠한 input에 대하여 0 ~ 1 사이의 output을 내는 함수이다. 신경망에서는 활성화함수로 sigmoid 함수를 이용해 신호를 변환하고 그 변환된 신호를 다음 *뉴런* 에 전달한다. \n",
    "\n",
    "퍼셉트론과 신경망의 주된 차이는 활성화 함수이고 그 밖에 여러층으로 이어지는 구조와 신호를 전달하는 방법은 기본적으로 앞에서 봤던 *perceptron* 과 같다.\n",
    "\n",
    "- - -\n",
    "\n",
    "### 계단 함수 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [False  True]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def step_func(x):\n",
    "    y = x > 0\n",
    "    print(y)\n",
    "    return y.astype(np.int)\n",
    "\n",
    "\n",
    "arr = np.array([[-1.0, 2.0], [0.0, 2.0]])\n",
    "step_func(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위는 numpy array로 선언된 변수를 계단 함수에 넣어 결과를 얻어낸 것이다. x로 각 행렬 원소들이 들어가게 되고 각 행렬의 shape에 맞춰 결과값이 나오게 된다.\n",
    "\n",
    "처음 부등호 연산을 시행할 때는 단순히 bool 형태가 나오게 되지만 이후 *astype()* 메소드를 통해서 해당 부분을 int 형으로 바꿔주게 된다. 따라서 결과값은 integer 형태를 얻을 수 있다.\n",
    "\n",
    "\n",
    "### 그래프\n",
    "\n",
    "계단함수의 그래프를 matplotlib 라이브러리를 이용해 그릴 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_func(x):\n",
    "    return np.array(x > 0, dtype = np.int)\n",
    "\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "# print(x.shape)\n",
    "\n",
    "y = step_func(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의처럼 계단함수의 그래프는 특정값 이상일 때 1 or 0의 값을 나타내게 된다.\n",
    "\n",
    "\n",
    "### sigmoid 함수 구현\n",
    "\n",
    "sigmoid는 계단함수보다 훨씬 더 유연한 형태를 띠고 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "\n",
    "# print(x.shape, y.shape)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비선형 함수\n",
    "\n",
    "계단 함수와 sigmoid 두 함수는 **비선형 함수** 이다. 신경망을 구성할 때는 활성화 함수로 비선형 함수를 사용해야 한다. 이는 선형함수를 사용하게 된다면 각 weight 값이 망의 깊이가 깊어짐에 따라 의미가 없어지기 때문이다. \n",
    "\n",
    "\n",
    "### ReLU 함수\n",
    "\n",
    "sigmoid, 계단 함수등을 설명했지만 최근에는 좀 더 다른 함수를 쓴다. *ReLU* 함수는 입력이 0을 넘으면 비례해서 출력을 해주고 0 이하라면 0을 출력해주는 함수이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.1,\n",
       "       0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3, 1.4,\n",
       "       1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7,\n",
       "       2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4. ,\n",
       "       4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "ReLU(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 형식으로 ReLU 함수를 선언할 수 있다.\n",
    "\n",
    "- - -\n",
    "\n",
    "## 다차원 배열의 계산\n",
    "\n",
    "### 다차원 배열 선언과 계산\n",
    "\n",
    "1 차원 배열과 2차원 배열을 np.array 함수를 통해서 선언할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[ 14 140]\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "dim1_arr = np.array([1, 2, 3])\n",
    "dim2_arr = np.array([[1, 2, 3], [10, 20, 30]])\n",
    "\n",
    "print(dim2_arr.shape)\n",
    "\n",
    "A = np.dot(dim2_arr, dim1_arr)\n",
    "print(dim2_arr.shape)\n",
    "print(dim1_arr.shape)\n",
    "\n",
    "print(A)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 배열들은 위와 같은 연산들이 가능하다. 그리고 신경망에서도 이러한 행렬의 곱을 사용할 수 있다. \n",
    "\n",
    "![Alt text](3_function_pulpan92_image/image2.png)\n",
    "\n",
    "위와 같은 방식으로 다층 신경망을 구현할 수도 있다.\n",
    "\n",
    "\n",
    "\n",
    "![Alt text](3_function_pulpan92_image/image3.png)\n",
    "\n",
    "각 layer마다 bias 값을 더하면서 각 계층의 퍼셉트론에 활성화함수가 적용된 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[210 420 630 840]\n",
      "(4,)\n",
      "[[210 420 630 840]]\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "X = np.array([10, 20])\n",
    "W1 = np.array([[1, 2, 3, 4], [10, 20, 30, 40]])\n",
    "b1 = 1\n",
    "\n",
    "result = np.dot(X, W1)\n",
    "\n",
    "print(result)\n",
    "print(result.shape)\n",
    "\n",
    "X = np.array([[10, 20]])\n",
    "W1 = np.array([[1, 2, 3, 4], [10, 20, 30, 40]])\n",
    "b1 = 10\n",
    "\n",
    "result = np.matmul(X, W1)\n",
    "\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위는 matmul과 dot을 구분지어서 결과를 낸 것이다. 각각의 결과를 보면 matmul의 경우 행렬의 곱셈을 나타내는 것으로 두 개 전부 data의 값은 같지만 matmul의 경우 하나의 벡터결과로 나타나게 되는것을 알 수 있다.\n",
    "\n",
    "\n",
    "### 다층 신경망\n",
    "\n",
    "![Alt text](3_function_pulpan92_image/image4.png)\n",
    "\n",
    "각 신경망의 weight 및 뉴런들은 위와 같이 도식화가 가능하다. 그리고 행렬의 곱을 이용해서 식을 간소화시킬 수 있다. 각 행렬들은 다음과 같이 나타나게 된다.\n",
    "\n",
    "![Alt text](3_function_pulpan92_image/image5.png)\n",
    "\n",
    "이를 numpy 모듈을 사용해서 구현할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] L1 shape:  (3,)\n",
      "[0.89090318 0.9168273  0.86989153]\n",
      "[] np.matmul:  [[2.1 2.4 1.9]]\n",
      "[] np.matmul shape:  (1, 3)\n",
      "[[0.89090318 0.9168273  0.86989153]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# layer1 np.dot process\n",
    "X = np.array([1, 2])\n",
    "W1 = np.array([[0.1, 0.2, 0.3], [0.5, 0.6, 0.3]])\n",
    "b1 = 1\n",
    "\n",
    "# layer 1 result; np.dot\n",
    "L1 = np.dot(X, W1) + b1\n",
    "print('[] L1 shape: ', L1.shape)\n",
    "\n",
    "# layer 2 np.dot process\n",
    "W2 = np.array([[0.1], [0.2], [0.3]])\n",
    "b2 = 0.1\n",
    "\n",
    "print(sigmoid(L1))\n",
    "\n",
    "# np.matmul process\n",
    "X = np.array([[1, 2]])\n",
    "W1 = np.array([[0.1, 0.2, 0.3], [0.5, 0.6, 0.3]])\n",
    "b1 = 1\n",
    "\n",
    "# layer 1 result; np.matmul\n",
    "L1 = np.matmul(X, W1) + b1\n",
    "\n",
    "print('[] np.matmul: ', L1)\n",
    "print('[] np.matmul shape: ', L1.shape)\n",
    "\n",
    "print(sigmoid(L1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer1을 np.dot, np.matmul을 이용해서 구현한 상태이다. 각 결과를 보면 데이터는 같지만 shape가 다르게 나오는 것을 알 수 있다. 그리고 활성화 함수를 은닉층의 뉴런에 배치해 실제 딥러닝 프레임워크의 layer 1을 표현하였다. \n",
    "\n",
    "![Alt text](3_function_pulpan92_image/image6.png)\n",
    "\n",
    "code는 입력층 부분에서 layer1으로 넘어가는 부분을 표현하였기에 이 후 행렬 값만 제대로 맞춰서 여러 layer를 가진 딥 러닝 구조를 만들 수 있다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] dot process result:  [0.74614674]\n",
      "[] dot result shape:  (1,)\n",
      "[] matmul process result:  [[0.74614674]]\n",
      "[] matmul result shape:  (1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def check(x):\n",
    "    print('[] shape: ', x.shape)\n",
    "\n",
    "# np.dot network init\n",
    "def dot_net_init():\n",
    "    net = {}\n",
    "    net['W1'] = np.array([[0.3, 0.1], [0.6, 0.1], [0.5, 0.2]])\n",
    "    net['b1'] = np.array([1, 1])\n",
    "    net['W2'] = np.array([[0.1, 0.1, 0.5], [0.8, 0.3, 0.2]])\n",
    "    net['b2'] = np.array([0.1, 0.4, 0.5])\n",
    "    net['W3'] = np.array([[0.3], [0.1], [0.4]])\n",
    "    net['b3'] = np.array([0.5])\n",
    "    return net\n",
    "\n",
    "def dot_process(X, net):\n",
    "    L1 = np.dot(X, net['W1']) + net['b1']\n",
    "    temp = sigmoid(L1)\n",
    "    L2 = np.dot(temp, net['W2']) + net['b2']\n",
    "    temp = sigmoid(L2)\n",
    "    L3 = np.dot(temp, net['W3']) + net['b3']\n",
    "    \n",
    "    return sigmoid(L3)\n",
    "    \n",
    "def matmul_process(X, net):\n",
    "    # for matrix operation\n",
    "    X = np.array([X])\n",
    "    \n",
    "    L1 = np.dot(X, net['W1']) + net['b1']\n",
    "    temp = sigmoid(L1)\n",
    "    L2 = np.dot(temp, net['W2']) + net['b2']\n",
    "    temp = sigmoid(L2)\n",
    "    L3 = np.dot(temp, net['W3']) + net['b3']\n",
    "    \n",
    "    return sigmoid(L3)\n",
    "\n",
    "def main():\n",
    "    # input matrix: (3,); np. dot process\n",
    "    X = np.array([1, 1, 1])\n",
    "    \n",
    "    # init neural network\n",
    "    net = dot_net_init()\n",
    "    \n",
    "    # np.dot process start\n",
    "    result = dot_process(X, net)\n",
    "    print('[] dot process result: ', result)\n",
    "    print('[] dot result shape: ', result.shape)\n",
    "    \n",
    "    # np.matmul process start\n",
    "    result = matmul_process(X, net)\n",
    "    print('[] matmul process result: ', result)\n",
    "    print('[] matmul result shape: ', result.shape)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서는 np.dot, np.matmul을 이용해서 3개의 layer로 이뤄진 딥 러닝을 구성해보았다\n",
    "\n",
    "**L1: 2\n",
    "L2: 3\n",
    "L3: 1**\n",
    "\n",
    "위와 같이 각 뉴럴이 구성되어 있으며 행은 input의 개수, 열은 output의 개수로 맞춰주면 되는 것을 알 수 있다. 그리고 각 계층 별로 result 값이 나오면 활성화함수 (*sigmoid, ReLU*) 를 사용해서 값을 재정비하고 그 이후 그 값을 다음 layer에 넣어줘야 한다.\n",
    "\n",
    "- - -\n",
    "\n",
    "\n",
    "## 출력층 설계\n",
    "\n",
    "기계 학습 문제에서는 결과를 **분류**, **회귀** 로 나누게 된다. **분류**는 데이터가 어느 class에 소속하느냐를 구분하는 문제이고 **회귀**는 입력데이터에서 연속적인 수치를 예측하는 문제이다.\n",
    "\n",
    "\n",
    "### Softmax 함수\n",
    "\n",
    "![Alt text](3_function_pulpan92_image/image7.png)\n",
    "\n",
    "softmax  함수는 식은 분류에서 사용한다. n은 *출력층의 뉴런 수* yk는 그 중 *k번 째 출력* 임을 뜻한다. softmax 함수를 쓰는 이유는 결과를 확률 값으로 해석하려 하기 위함이다. \n",
    "\n",
    "이는 각 뉴런들의 활성화 함수와는 달리 최종 출력계층에서 작용하여 각 결과를 확률로 보내는데 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n",
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    print(exp_x)\n",
    "    s = np.sum(exp_x, 0)\n",
    "    \n",
    "    y = exp_x / s\n",
    "    \n",
    "    return y\n",
    "\n",
    "test = np.array([1, 2, 3])\n",
    "print(softmax(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 softmax는 구현이 잘 되지만 기본적으로 사용되는 exp 함수는 지수함수로 나타나기 때문에 작은 값에도 결과값은 아주 큰 값이 나올 수 있다. 그렇다면 NaN이라는 결과가 나오게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf inf inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pulpan92\\Anaconda3\\envs\\machine\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\pulpan92\\Anaconda3\\envs\\machine\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([1000, 1001, 2000])\n",
    "\n",
    "softmax(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 방지하려면 input으로 들어가는 행렬 중 가장 큰 원소를 뺀 후 그 값을 softmax에 넣으면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 -10 -20]\n",
      "[1.00000000e+00 4.53999298e-05 2.06115362e-09]\n",
      "[9.99954600e-01 4.53978686e-05 2.06106005e-09]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(X):\n",
    "    maxnum = np.max(X)\n",
    "    print(X - maxnum)\n",
    "    exp_x = np.exp(X - maxnum)\n",
    "    print(exp_x)\n",
    "    s = np.sum(exp_x, 0)\n",
    "    \n",
    "    y = exp_x / s\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "X = np.array([1010, 1000, 990])\n",
    "\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax 함수의 가장 큰 특징은 모든 확률의 값이 1인 것이다. 따라서 출력층에서 softmax를 거쳐 나온 결과물들을 확률로 해석할 수 있다. \n",
    "\n",
    "- - -\n",
    "\n",
    "## 손글씨 인식\n",
    "\n",
    "### MNIST dataset\n",
    "\n",
    "손글씨 숫자 이미지 집합을 나타낸 것이다. 이는 총 train image 60000장, test image 10000장이 있고 각각의 label이 따로 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n",
    "batch = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위는 tensorflow module에서 train의 각 100개의 image와 label을 갖고오게 된다. batch[0]은 image, batch[1]은 label 데이터들이 담겨져 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def get_data():\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n",
    "    batch = mnist.train.next_batch(10000)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "def init_net():\n",
    "    net = {}\n",
    "    net['W1'] = np.array\n",
    "\n",
    "def main():\n",
    "    mnist = get_data()\n",
    "    print(mnist[0][1].shape)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "machine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
